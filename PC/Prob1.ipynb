{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=not-callable\n",
    "# pylint: disable=no-member\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import functions as F\n",
    "\n",
    "\n",
    "def set_tensor(arr, device):\n",
    "    return torch.from_numpy(arr).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PredictiveCodingNetwork(object):\n",
    "    def __init__(self, cf):\n",
    "        self.device = cf.device\n",
    "        self.n_layers = cf.n_layers\n",
    "        self.act_fn = cf.act_fn\n",
    "        self.neurons = cf.neurons\n",
    "        self.vars = cf.vars.float().to(self.device)\n",
    "        self.itr_max = cf.itr_max\n",
    "        self.batch_size = cf.batch_size\n",
    "\n",
    "        self.beta_1 = cf.beta_1\n",
    "        self.beta_2 = cf.beta_2\n",
    "        self.beta = cf.beta\n",
    "        self.div = cf.div\n",
    "        self.d_rate = cf.d_rate\n",
    "        self.l_rate = cf.l_rate\n",
    "        self.condition = cf.condition / (sum(cf.neurons) - cf.neurons[0])\n",
    "\n",
    "        self.optim = cf.optim\n",
    "        self.eps = cf.eps\n",
    "        self.decay_r = cf.decay_r\n",
    "        self.c_b = [[] for _ in range(self.n_layers)]\n",
    "        self.c_w = [[] for _ in range(self.n_layers)]\n",
    "        self.v_b = [[] for _ in range(self.n_layers)]\n",
    "        self.v_w = [[] for _ in range(self.n_layers)]\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self._init_params()\n",
    "\n",
    "    def train_epoch(self, x_batches, y_batches, epoch_num=None):\n",
    "        n_batches = len(x_batches)\n",
    "        for batch_id, (x_batch, y_batch) in enumerate(zip(x_batches, y_batches)):\n",
    "\n",
    "            if batch_id % 500 == 0 and batch_id > 0:\n",
    "                print(f\"batch {batch_id}\")\n",
    "\n",
    "            x_batch = set_tensor(x_batch, self.device)\n",
    "            y_batch = set_tensor(y_batch, self.device)\n",
    "            batch_size = x_batch.size(1)   \n",
    "\n",
    "            x = [[] for _ in range(self.n_layers)]\n",
    "            x[0] = x_batch\n",
    "            for l in range(1, self.n_layers):\n",
    "                b = self.b[l - 1].repeat(1, batch_size)\n",
    "                x[l] = self.W[l - 1] @ F.f(x[l - 1], self.act_fn) + b   \n",
    "            x[self.n_layers - 1] = y_batch\n",
    "\n",
    "            x, errors, _ = self.infer(x, batch_size)\n",
    "            self.update_params(\n",
    "                x, errors, batch_size, epoch_num=epoch_num, n_batches=n_batches, curr_batch=batch_id\n",
    "            )\n",
    "\n",
    "            if batch_id == 1:\n",
    "                plot_imgs(x_batch, \"D:/Documents/documents/LectureMaterial/2022_1/BIM/ProjectC/Term C_2022_distribution/plot_imgs\"+str(epoch_num)+\".png\")\n",
    "\n",
    "\n",
    "    def test_epoch(self, x_batches, y_batches):\n",
    "        accs = []\n",
    "        for x_batch, y_batch in zip(x_batches, y_batches):\n",
    "            x_batch = set_tensor(x_batch, self.device)\n",
    "            y_batch = set_tensor(y_batch, self.device)\n",
    "            batch_size = x_batch.size(1)\n",
    "\n",
    "            x = [[] for _ in range(self.n_layers)]\n",
    "            x[0] = x_batch\n",
    "            for l in range(1, self.n_layers):\n",
    "                b = self.b[l - 1].repeat(1, batch_size)\n",
    "                x[l] = self.W[l - 1] @ F.f(x[l - 1], self.act_fn) + b\n",
    "            pred_y = x[-1]\n",
    "\n",
    "            acc = mnist_accuracy(pred_y, y_batch)\n",
    "            accs.append(acc)\n",
    "        return accs\n",
    "\n",
    "    def generate_data(self, x_batch):\n",
    "        x_batch = set_tensor(x_batch, self.device)\n",
    "        batch_size = x_batch.size(1)\n",
    "\n",
    "        x = [[] for _ in range(self.n_layers)]\n",
    "        x[0] = x_batch\n",
    "        for l in range(1, self.n_layers):\n",
    "            b = self.b[l - 1].repeat(1, batch_size)\n",
    "            x[l] = self.W[l - 1] @ F.f(x[l - 1], self.act_fn) + b\n",
    "        pred_y = x[-1]\n",
    "        return pred_y\n",
    "\n",
    "    def infer(self, x, batch_size):\n",
    "        mu = [[] for _ in range(self.n_layers)]\n",
    "        errors = [[] for _ in range(self.n_layers)]\n",
    "        f_x_arr = [[] for _ in range(self.n_layers)]\n",
    "        f_x_deriv_arr = [[] for _ in range(self.n_layers)]\n",
    "        f_0 = 0\n",
    "        its = 0\n",
    "        beta = self.beta  # 0.1\n",
    "\n",
    "        for l in range(1, self.n_layers):\n",
    "            f_x = F.f(x[l - 1], self.act_fn)\n",
    "            f_x_deriv = F.f_deriv(x[l - 1], self.act_fn)\n",
    "            f_x_arr[l - 1] = f_x\n",
    "            f_x_deriv_arr[l - 1] = f_x_deriv\n",
    "\n",
    "            b = self.b[l - 1].repeat(1, batch_size)\n",
    "            mu[l] = self.W[l - 1] @ F.f(x[l - 1], self.act_fn) + b\n",
    "            errors[l] = (x[l] - mu[l]) / self.vars[l]\n",
    "            f_0 = f_0 + (-0.5) * sum(((x[l] - b)**2)/self.vars[l])\n",
    "\n",
    "        for itr in range(self.itr_max):  # 0~49\n",
    "            # update node activity\n",
    "            for l in range(1, self.n_layers - 1): # 1,2\n",
    "\n",
    "                x[l] = errors[l] + self.W[l-1]@f_x_arr[l-1] \n",
    "\n",
    "            # update errors\n",
    "            f = 0\n",
    "            for l in range(1, self.n_layers):\n",
    "                f_x = F.f(x[l - 1], self.act_fn)\n",
    "                f_x_deriv = F.f_deriv(x[l - 1], self.act_fn)\n",
    "                f_x_arr[l - 1] = f_x\n",
    "                f_x_deriv_arr[l - 1] = f_x_deriv\n",
    "                b = self.b[l - 1].repeat(1, batch_size) \n",
    "                mu[l] = self.W[l - 1] @ F.f(x[l - 1], self.act_fn) + b\n",
    "                errors[l] = (x[l] - mu[l]) / self.vars[l]\n",
    "                f = f + (-0.5) * sum(((x[l] - b)**2)/self.vars[l])\n",
    "\n",
    "            diff = f - f_0\n",
    "            threshold = self.condition * self.beta / self.vars[self.n_layers - 1] \n",
    "            if torch.any(diff < 0):\n",
    "                beta = beta / self.div\n",
    "            elif torch.mean(diff) < threshold:\n",
    "                break\n",
    "\n",
    "            f_0 = f\n",
    "            its = itr\n",
    "\n",
    "        return x, errors, its\n",
    "\n",
    "    def update_params(self, x, errors, batch_size, epoch_num=None, n_batches=None, curr_batch=None):\n",
    "        grad_w = [[] for _ in range(self.n_layers - 1)]\n",
    "        grad_b = [[] for _ in range(self.n_layers - 1)]\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            grad_w[l] = errors[l+1] @ torch.transpose(F.f(x[l], self.act_fn), 0, 1)\n",
    "            grad_b[l] = self.vars[-1] * (1 / batch_size) * torch.sum(errors[l + 1], axis=1)\n",
    "\n",
    "        self._apply_gradients(grad_w, grad_b, epoch_num=epoch_num, n_batches=n_batches, curr_batch=curr_batch)\n",
    "\n",
    "    def _init_params(self):\n",
    "        weights = [[] for _ in range(self.n_layers)]\n",
    "        bias = [[] for _ in range(self.n_layers)]\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            norm_b = 0\n",
    "            if self.act_fn is F.LINEAR:\n",
    "                norm_w = np.sqrt(1 / (self.neurons[l + 1] + self.neurons[l]))\n",
    "            elif self.act_fn is F.TANH:\n",
    "                norm_w = np.sqrt(6 / (self.neurons[l + 1] + self.neurons[l]))\n",
    "            elif self.act_fn is F.LOGSIG:\n",
    "                norm_w = 4 * np.sqrt(6 / (self.neurons[l + 1] + self.neurons[l]))\n",
    "            else:\n",
    "                raise ValueError(f\"{self.act_fn} not supported\")\n",
    "\n",
    "            layer_w = np.random.uniform(-1, 1, size=(self.neurons[l + 1], self.neurons[l])) * norm_w\n",
    "            layer_b = np.zeros((self.neurons[l + 1], 1)) + norm_b * np.ones((self.neurons[l + 1], 1))\n",
    "            weights[l] = set_tensor(layer_w, self.device)\n",
    "            bias[l] = set_tensor(layer_b, self.device)\n",
    "\n",
    "        self.W = weights\n",
    "        self.b = bias\n",
    "\n",
    "        for l in range(self.n_layers - 1):\n",
    "            self.c_b[l] = torch.zeros_like(self.b[l])\n",
    "            self.c_w[l] = torch.zeros_like(self.W[l])\n",
    "            self.v_b[l] = torch.zeros_like(self.b[l])\n",
    "            self.v_w[l] = torch.zeros_like(self.W[l])\n",
    "\n",
    "    def _apply_gradients(self, grad_w, grad_b, epoch_num=None, n_batches=None, curr_batch=None):\n",
    "\n",
    "        if self.optim is \"RMSPROP\":\n",
    "            for l in range(self.n_layers - 1):\n",
    "                grad_b[l] = grad_b[l].unsqueeze(dim=1)\n",
    "                self.c_w[l] = self.decay_r * self.c_w[l] + (1 - self.decay_r) * grad_w[l] ** 2\n",
    "                self.c_b[l] = self.decay_r * self.c_b[l] + (1 - self.decay_r) * grad_b[l] ** 2\n",
    "\n",
    "                self.W[l] = self.W[l] + self.l_rate * (grad_w[l] / (torch.sqrt(self.c_w[l]) + self.eps))\n",
    "                self.b[l] = self.b[l] + self.l_rate * (grad_b[l] / (torch.sqrt(self.c_b[l]) + self.eps))\n",
    "\n",
    "        elif self.optim is \"ADAM\":\n",
    "            for l in range(self.n_layers - 1):\n",
    "                grad_b[l] = grad_b[l].unsqueeze(dim=1)\n",
    "                self.c_b[l] = self.beta_1 * self.c_b[l] + (1 - self.beta_1) * grad_b[l]\n",
    "                self.c_w[l] = self.beta_1 * self.c_w[l] + (1 - self.beta_1) * grad_w[l]\n",
    "\n",
    "                self.v_b[l] = self.beta_2 * self.v_b[l] + (1 - self.beta_2) * grad_b[l] ** 2\n",
    "                self.v_w[l] = self.beta_2 * self.v_w[l] + (1 - self.beta_2) * grad_w[l] ** 2\n",
    "\n",
    "                t = (epoch_num) * n_batches + curr_batch\n",
    "                self.W[l] = self.W[l] + self.l_rate * np.sqrt(1 - self.beta_2 ** t) * self.c_w[l] / (\n",
    "                    torch.sqrt(self.v_w[l]) + self.eps\n",
    "                )\n",
    "                self.b[l] = self.b[l] + self.l_rate * np.sqrt(1 - self.beta_2 ** t) * self.c_b[l] / (\n",
    "                    torch.sqrt(self.v_b[l]) + self.eps\n",
    "                )\n",
    "\n",
    "        elif self.optim is \"SGD\" or self.optim is None:\n",
    "            for l in range(self.n_layers - 1):\n",
    "                self.W[l] = self.W[l] + self.l_rate * grad_w[l]\n",
    "                self.b[l] = self.b[l] + self.l_rate * grad_b[l].unsqueeze(dim=1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{self.optim} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mnist_train_set():\n",
    "    #return torchvision.datasets.MNIST(\"MNIST_train\", download=True, train=True)\n",
    "    return torchvision.datasets.MNIST(\"MNIST_train\", download=True, train=True)\n",
    "\n",
    "\n",
    "def get_mnist_test_set():\n",
    "    return torchvision.datasets.MNIST(\"MNIST_test\", download=True, train=False)\n",
    "\n",
    "\n",
    "def onehot(label, n_classes=10):\n",
    "    arr = np.zeros([10])\n",
    "    arr[int(label)] = 1.0\n",
    "    return arr\n",
    "\n",
    "\n",
    "def img_to_np(img):\n",
    "    return np.array(img).reshape([784]) / 255.0\n",
    "\n",
    "\n",
    "def get_imgs(dataset):\n",
    "    imgs = np.array([img_to_np(dataset[i][0]) for i in range(len(dataset))])\n",
    "    return np.swapaxes(imgs, 0, 1)\n",
    "\n",
    "\n",
    "def get_labels(dataset):\n",
    "    labels = np.array([onehot(dataset[i][1]) for i in range(len(dataset))])\n",
    "    return np.swapaxes(labels, 0, 1)\n",
    "\n",
    "\n",
    "def scale_imgs(imgs, scale_factor):\n",
    "    return imgs * scale_factor + 0.5 * (1 - scale_factor) * np.ones(imgs.shape)\n",
    "\n",
    "\n",
    "def scale_labels(labels, scale_factor):\n",
    "    return labels * scale_factor + 0.5 * (1 - scale_factor) * np.ones(labels.shape)\n",
    "\n",
    "\n",
    "def mnist_accuracy(pred_labels, labels):\n",
    "    correct = 0\n",
    "    batch_size = pred_labels.size(1)\n",
    "    for b in range(batch_size):\n",
    "        if torch.argmax(pred_labels[:, b]) == torch.argmax(labels[:, b]):\n",
    "            correct += 1\n",
    "    return correct / batch_size\n",
    "\n",
    "\n",
    "def get_batches(imgs, labels, batch_size):\n",
    "    n_data = imgs.shape[1]   # 60000\n",
    "    n_batches = int(np.ceil(n_data / batch_size))\n",
    "\n",
    "    img_batches = [[] for _ in range(n_batches)]\n",
    "    label_batches = [[] for _ in range(n_batches)]\n",
    "\n",
    "    for batch in range(n_batches):\n",
    "        if batch == n_batches - 1:\n",
    "            start = batch * batch_size\n",
    "            img_batches[batch] = imgs[:, start:]\n",
    "            label_batches[batch] = labels[:, start:]\n",
    "        else:\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            img_batches[batch] = imgs[:, start:end]\n",
    "            label_batches[batch] = labels[:, start:end]\n",
    "\n",
    "    return img_batches, label_batches\n",
    "\n",
    "\n",
    "def plot_imgs(img_batch, save_path):\n",
    "    img_batch = img_batch.detach().cpu().numpy()\n",
    "    batch_size = img_batch.shape[1]\n",
    "    dim = nearest_square(batch_size) # 15\n",
    "\n",
    "    imgs = [np.reshape(img_batch[:, i], [28, 28]) for i in range(dim ** 2)]  #784 array to 28 x 28 image\n",
    "    _, axes = plt.subplots(dim, dim)\n",
    "    axes = axes.flatten()\n",
    "    for i, img in enumerate(imgs):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_axis_off()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def nearest_square(limit):\n",
    "    answer = 0\n",
    "    while (answer + 1) ** 2 < limit:\n",
    "        answer += 1\n",
    "    return answer\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "\n",
    "\n",
    "def main(cf):\n",
    "    print(f\"device [{cf.device}]\")\n",
    "    print(\"loading MNIST data...\")\n",
    "    train_set = get_mnist_train_set()\n",
    "    test_set = get_mnist_test_set()\n",
    "\n",
    "    img_train = get_imgs(train_set)\n",
    "    img_test = get_imgs(test_set)\n",
    "    label_train = get_labels(train_set)\n",
    "    label_test = get_labels(test_set)\n",
    "\n",
    "    if cf.data_size is not None:\n",
    "        test_size = cf.data_size // 5\n",
    "        img_train = img_train[:, 0 : cf.data_size]\n",
    "        label_train = label_train[:, 0 : cf.data_size]\n",
    "        img_test = img_test[:, 0:test_size]\n",
    "        label_test = label_test[:, 0:test_size]\n",
    "\n",
    "    msg = \"img_train {} img_test {} label_train {} label_test {}\"\n",
    "    print(msg.format(img_train.shape, img_test.shape, label_train.shape, label_test.shape))\n",
    "\n",
    "    print(\"performing preprocessing...\")\n",
    "    if cf.apply_scaling:\n",
    "        img_train = scale_imgs(img_train, cf.img_scale)\n",
    "        img_test = scale_imgs(img_test, cf.img_scale)\n",
    "        label_train = scale_labels(label_train, cf.label_scale)\n",
    "        label_test = scale_labels(label_test, cf.label_scale)\n",
    "\n",
    "    if cf.apply_inv:\n",
    "        img_train = F.f_inv(img_train, cf.act_fn)\n",
    "        img_test = F.f_inv(img_test, cf.act_fn)\n",
    "\n",
    "    model = PredictiveCodingNetwork(cf)\n",
    "\n",
    "    test_accuracy_epoch = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for epoch in range(cf.n_epochs):\n",
    "            print(f\"\\nepoch {epoch}\")\n",
    "\n",
    "            img_batches, label_batches = get_batches(img_train, label_train, cf.batch_size)\n",
    "            print(f\"training on {len(img_batches)} batches of size {cf.batch_size}\")\n",
    "            model.train_epoch(img_batches, label_batches, epoch_num=epoch)\n",
    "            \n",
    "            img_batches, label_batches = get_batches(img_test, label_test, cf.batch_size)\n",
    "            print(f\"testing on {len(img_batches)} batches of size {cf.batch_size}\")\n",
    "            accs = model.test_epoch(img_batches, label_batches)\n",
    "            print(f\"average accuracy {np.mean(np.array(accs))}\")\n",
    "            test_accuracy_epoch.append(np.mean(np.array(accs)))\n",
    "            perm = np.random.permutation(img_train.shape[1])\n",
    "            img_train = img_train[:, perm]\n",
    "            label_train = label_train[:, perm]\n",
    "\n",
    "    plt.plot(test_accuracy_epoch)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device [cuda]\n",
      "loading MNIST data...\n",
      "img_train (784, 60000) img_test (784, 10000) label_train (10, 60000) label_test (10, 10000)\n",
      "performing preprocessing...\n",
      "\n",
      "epoch 0\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8240000000000001\n",
      "\n",
      "epoch 1\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8553999999999999\n",
      "\n",
      "epoch 2\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8709\n",
      "\n",
      "epoch 3\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8700999999999999\n",
      "\n",
      "epoch 4\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8781\n",
      "\n",
      "epoch 5\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8766000000000002\n",
      "\n",
      "epoch 6\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8783000000000001\n",
      "\n",
      "epoch 7\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8823000000000002\n",
      "\n",
      "epoch 8\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8817999999999999\n",
      "\n",
      "epoch 9\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8870999999999999\n",
      "\n",
      "epoch 10\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8850000000000001\n",
      "\n",
      "epoch 11\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8855999999999998\n",
      "\n",
      "epoch 12\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.885\n",
      "\n",
      "epoch 13\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8885\n",
      "\n",
      "epoch 14\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8905\n",
      "\n",
      "epoch 15\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8881999999999999\n",
      "\n",
      "epoch 16\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8934999999999998\n",
      "\n",
      "epoch 17\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8891000000000001\n",
      "\n",
      "epoch 18\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.895\n",
      "\n",
      "epoch 19\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8917000000000002\n",
      "\n",
      "epoch 20\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8911999999999999\n",
      "\n",
      "epoch 21\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8967\n",
      "\n",
      "epoch 22\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.892\n",
      "\n",
      "epoch 23\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8966999999999998\n",
      "\n",
      "epoch 24\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8931\n",
      "\n",
      "epoch 25\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8942999999999999\n",
      "\n",
      "epoch 26\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8957\n",
      "\n",
      "epoch 27\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8964000000000002\n",
      "\n",
      "epoch 28\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8945000000000001\n",
      "\n",
      "epoch 29\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8996\n",
      "\n",
      "epoch 30\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8991\n",
      "\n",
      "epoch 31\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8964\n",
      "\n",
      "epoch 32\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8968\n",
      "\n",
      "epoch 33\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8963000000000001\n",
      "\n",
      "epoch 34\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8957\n",
      "\n",
      "epoch 35\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8996000000000001\n",
      "\n",
      "epoch 36\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8967\n",
      "\n",
      "epoch 37\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8976999999999999\n",
      "\n",
      "epoch 38\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8991\n",
      "\n",
      "epoch 39\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9012\n",
      "\n",
      "epoch 40\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9006999999999998\n",
      "\n",
      "epoch 41\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8980000000000001\n",
      "\n",
      "epoch 42\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9014000000000002\n",
      "\n",
      "epoch 43\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8994\n",
      "\n",
      "epoch 44\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8982000000000001\n",
      "\n",
      "epoch 45\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9013000000000001\n",
      "\n",
      "epoch 46\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8995000000000001\n",
      "\n",
      "epoch 47\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.899\n",
      "\n",
      "epoch 48\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.904\n",
      "\n",
      "epoch 49\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8982000000000001\n",
      "\n",
      "epoch 50\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9011\n",
      "\n",
      "epoch 51\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9021000000000001\n",
      "\n",
      "epoch 52\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8986\n",
      "\n",
      "epoch 53\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9011\n",
      "\n",
      "epoch 54\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8984000000000001\n",
      "\n",
      "epoch 55\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9016\n",
      "\n",
      "epoch 56\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9015000000000001\n",
      "\n",
      "epoch 57\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9003\n",
      "\n",
      "epoch 58\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8998999999999999\n",
      "\n",
      "epoch 59\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9014\n",
      "\n",
      "epoch 60\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9030000000000001\n",
      "\n",
      "epoch 61\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9025\n",
      "\n",
      "epoch 62\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9044\n",
      "\n",
      "epoch 63\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9020999999999999\n",
      "\n",
      "epoch 64\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8998000000000002\n",
      "\n",
      "epoch 65\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9017000000000001\n",
      "\n",
      "epoch 66\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9031\n",
      "\n",
      "epoch 67\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9047\n",
      "\n",
      "epoch 68\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9017000000000002\n",
      "\n",
      "epoch 69\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9042\n",
      "\n",
      "epoch 70\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.8999999999999999\n",
      "\n",
      "epoch 71\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.902\n",
      "\n",
      "epoch 72\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9018999999999999\n",
      "\n",
      "epoch 73\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9061999999999999\n",
      "\n",
      "epoch 74\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9041000000000001\n",
      "\n",
      "epoch 75\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9028\n",
      "\n",
      "epoch 76\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9058000000000002\n",
      "\n",
      "epoch 77\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9029999999999998\n",
      "\n",
      "epoch 78\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9016\n",
      "\n",
      "epoch 79\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9020999999999999\n",
      "\n",
      "epoch 80\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9044999999999999\n",
      "\n",
      "epoch 81\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9017999999999999\n",
      "\n",
      "epoch 82\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.904\n",
      "\n",
      "epoch 83\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9044000000000001\n",
      "\n",
      "epoch 84\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.905\n",
      "\n",
      "epoch 85\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.902\n",
      "\n",
      "epoch 86\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9024\n",
      "\n",
      "epoch 87\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9041000000000001\n",
      "\n",
      "epoch 88\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.905\n",
      "\n",
      "epoch 89\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9021000000000001\n",
      "\n",
      "epoch 90\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9037\n",
      "\n",
      "epoch 91\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9041000000000001\n",
      "\n",
      "epoch 92\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9048000000000002\n",
      "\n",
      "epoch 93\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9057\n",
      "\n",
      "epoch 94\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9035\n",
      "\n",
      "epoch 95\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9009\n",
      "\n",
      "epoch 96\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9019000000000001\n",
      "\n",
      "epoch 97\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9035000000000002\n",
      "\n",
      "epoch 98\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9045000000000001\n",
      "\n",
      "epoch 99\n",
      "training on 300 batches of size 200\n",
      "testing on 50 batches of size 200\n",
      "average accuracy 0.9011999999999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cf = AttrDict()\n",
    "\n",
    "    cf.n_epochs = 100\n",
    "    cf.data_size = None\n",
    "    cf.batch_size = 200   # 128 -> 200\n",
    "\n",
    "    cf.apply_inv = True\n",
    "    cf.apply_scaling = True\n",
    "    cf.label_scale = 0.94\n",
    "    cf.img_scale = 1.0\n",
    "\n",
    "    cf.neurons = [784, 500, 500, 10]\n",
    "    cf.n_layers = len(cf.neurons)\n",
    "    cf.act_fn = F.TANH\n",
    "    cf.var_out = 1\n",
    "    cf.vars = torch.ones(cf.n_layers)\n",
    "\n",
    "    cf.itr_max = 50\n",
    "    cf.beta = 0.1\n",
    "    cf.div = 2\n",
    "    cf.condition = 1e-6\n",
    "    cf.d_rate = 0\n",
    "\n",
    "    # optim parameters\n",
    "    cf.l_rate = 1e-3      \n",
    "    cf.optim = \"ADAM\"\n",
    "    cf.eps = 1e-8\n",
    "    cf.decay_r = 0.9\n",
    "    cf.beta_1 = 0.9\n",
    "    cf.beta_2 = 0.999\n",
    "\n",
    "    cf.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    main(cf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27bf4fb6fe7b50df750aee21a6b37bb7129eea65c5247ab3402e19f21a6df50f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
