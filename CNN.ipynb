{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XaMMVxEvBgV"
   },
   "source": [
    "## Medical image diagnosis challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_m8nRxjjES-"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf00Wc9Ui_ig"
   },
   "source": [
    "### 2. Set hyperparameters\n",
    "* About batch size & epoch\n",
    "    * https://machine-learning.paperspace.com/wiki/epoch#:~:text=The%20number%20of%20iterations%20is,to%20complete%20a%20single%20epoch.\n",
    "* There are other parts of the code that define hyperparameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Mnl0JpzjE0x"
   },
   "outputs": [],
   "source": [
    "batch_size=800 \n",
    "Epoch = 100 \n",
    "Learning_rate=0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-WewHcyvK4i"
   },
   "source": [
    "### 3. Load and preprocess the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T48fDuPFvLDq"
   },
   "outputs": [],
   "source": [
    "file_path=\"./data/\"  # You must change file_path \n",
    "\n",
    "original_training_data=[]\n",
    "test_img=[]\n",
    "\n",
    "for i in range(400):\n",
    "    raw_image_normal=Image.open(file_path+\"training_images/normal\"+str(i+1)+'.png')\n",
    "    raw_image_normal=np.array(raw_image_normal)/255\n",
    "    raw_image_normal = raw_image_normal[np.newaxis, :, :]\n",
    "    original_training_data.append([raw_image_normal, [1,0]])\n",
    "\n",
    "    raw_image_abnormal=Image.open(file_path+\"training_images/abnormal\"+str(i+1)+'.png')\n",
    "    raw_image_abnormal=np.array(raw_image_abnormal)/255\n",
    "    raw_image_abnormal=raw_image_abnormal[np.newaxis,:,:]\n",
    "    original_training_data.append([raw_image_abnormal,[0,1]])\n",
    "\n",
    "for i in range(50):\n",
    "    raw_image_test=Image.open(file_path+\"test_images/\"+str(i+1)+'.png')\n",
    "    raw_image_test=np.array(raw_image_test)/255\n",
    "    raw_image_test = raw_image_test[np.newaxis, :, :]\n",
    "    test_img.append(raw_image_test)\n",
    "    \n",
    "print(len(original_training_data)) # trianing set size\n",
    "print(len(test_img))  # test set size\n",
    "print(test_img[0].shape) # input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3G8CkLwvQ4l"
   },
   "source": [
    "### 4. Separate the dataset (training set & validation set)\n",
    "\n",
    "* The size of the validation set is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJjmUa3VvOnY"
   },
   "outputs": [],
   "source": [
    "random.shuffle(original_training_data)\n",
    "selected_training_data=original_training_data[:360] \n",
    "validation_data=original_training_data[360:] \n",
    "\n",
    "print(len(selected_training_data)) # training set size\n",
    "print(len(validation_data)) # validation set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PM5XXMgyG96"
   },
   "source": [
    "### 5. Define functions: batch sampling & validation data extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1lTpcN6vSJ8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def batch_sampling (num):\n",
    "    global selected_training_data\n",
    "\n",
    "    if num==0:\n",
    "        random.shuffle(selected_training_data)\n",
    "\n",
    "    x_size=0\n",
    "    train_x=[]\n",
    "    train_y=[]\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            train_x.append(selected_training_data[num*batch_size+i][0])\n",
    "            train_y.append(selected_training_data[num*batch_size+i][1])\n",
    "            x_size+=1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return torch.from_numpy(np.array(train_x)).float(), torch.from_numpy(np.array(train_y)).long(), x_size\n",
    "\n",
    "def extract_validation_data ():\n",
    "    global validation_data\n",
    "    x_size = 0\n",
    "    val_x=[]\n",
    "    val_y=[]\n",
    "    for i in range(len(validation_data)):\n",
    "        val_x.append(validation_data[i][0])\n",
    "        val_y.append(validation_data[i][1])\n",
    "        x_size += 1\n",
    "    return torch.from_numpy(np.array(val_x)).float(), torch.from_numpy(np.array(val_y)).long(), x_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi425up6y4JU"
   },
   "source": [
    "### 6. Define a neural network\n",
    "\n",
    "* You can freely modify hyperparameters in the below part (number of layers, number of units, size of the kernel, etc.).\n",
    "* You don't have to use a CNN. Try any other models if you want.\n",
    "* For more details about the network components, check https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhnalP0-vYSG"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5,stride = 1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 4, kernel_size = 3,stride = 1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(-1, 32 * 32 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "net = Net() # make an instance\n",
    "print(net) # check the architecture\n",
    "print(net(torch.randn(10,1,128,128))) # see the network works correctly with an arbitrary input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu_mmMSyzOjI"
   },
   "source": [
    "### 7. Define a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYp4VRDTvfmi"
   },
   "outputs": [],
   "source": [
    "def criterion(y_pred, y_true):\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=Learning_rate) \n",
    "optimizer.state = collections.defaultdict(dict) # network initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsNsklFMzO9K"
   },
   "source": [
    "### 8. Train the network and print the test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xw30MO4UvhhQ"
   },
   "outputs": [],
   "source": [
    "batch_number=int((len(selected_training_data)-0.5)/batch_size)+1 \n",
    "train_loss_epoch = [] \n",
    "val_loss_epoch = [] \n",
    "train_acc_epoch = [] \n",
    "val_acc_epoch = [] \n",
    "for epoch in range(Epoch):\n",
    "    net.train()\n",
    "    for batch in range(batch_number):\n",
    "        train_x, train_y, x_size = batch_sampling(batch)\n",
    "        optimizer.zero_grad()  # Gradient Reset\n",
    "        output = net(train_x)\n",
    "\n",
    "        loss = criterion(output, train_y)\n",
    "\n",
    "        loss.backward()  # Back propagation\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        print(\"Epoch \", epoch)\n",
    "        training_loss=0\n",
    "        training_accuracy=0\n",
    "        for batch in range(batch_number):\n",
    "            train_x, train_y, x_size = batch_sampling(batch)\n",
    "            output = net(train_x)\n",
    "\n",
    "            training_loss += x_size*criterion(output, train_y).item()\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            _, label_index = torch.max(train_y, 1)\n",
    "            training_accuracy+=(predicted == label_index).sum().item()\n",
    "\n",
    "        training_accuracy=training_accuracy/len(selected_training_data)\n",
    "        training_loss=training_loss/len(selected_training_data)\n",
    "        train_acc_epoch.append(training_accuracy)\n",
    "        train_loss_epoch.append(training_loss) \n",
    "        \n",
    "        print(\"Training loss (average) :\", training_loss)\n",
    "        print(\"Training accuracy       :\", training_accuracy * 100,\"%\")\n",
    "\n",
    "        validation_loss=0\n",
    "        validation_accuracy=0\n",
    "        val_x, val_y, x_size=extract_validation_data()\n",
    "        output = net(val_x)\n",
    "\n",
    "        validation_loss += x_size * criterion(output, val_y).item()\n",
    "\n",
    "        _, predicted_v = torch.max(output.data, 1)\n",
    "        _, label_index_v = torch.max(val_y, 1)\n",
    "        validation_accuracy += (predicted_v == label_index_v).sum().item()\n",
    "        validation_accuracy=validation_accuracy/len(validation_data)\n",
    "        validation_loss = validation_loss / len(validation_data)\n",
    "        val_acc_epoch.append(validation_accuracy)\n",
    "        val_loss_epoch.append(validation_loss)\n",
    "\n",
    "        print(\"Validation loss (average) :\", validation_loss)\n",
    "        print(\"Validation accuracy       :\", validation_accuracy * 100, \"%\")\n",
    "\n",
    "        output = net(torch.from_numpy(np.array(test_img)).float())\n",
    "        _, predicted_t = torch.max(output.data, 1)\n",
    "        print(predicted_t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axs[0].plot(train_loss_epoch, label = 'train') \n",
    "axs[0].plot(val_loss_epoch, label = 'val') \n",
    "axs[0].set_title('Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(train_acc_epoch, label = 'train') \n",
    "axs[1].plot(val_acc_epoch, label = 'val') \n",
    "axs[1].set_title('Accuracy')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project_B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
